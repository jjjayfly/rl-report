\documentclass[12pt,a4paper,oneside]{book}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{latexsym}
%\usepackage{pslatex}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{tocloft}
\usepackage{multirow}
\usepackage{fontspec}
\usepackage{booktabs}
\usepackage{makecell}%修改表格宽度
\usepackage{caption}
% Times New Roman
\setromanfont[
BoldFont=Times New Roman Bold.ttf,
ItalicFont=Times New Roman Italic.ttf,
BoldItalicFont=Times New Roman Bold Italic.ttf,
]{Times New Roman.ttf}
%\setmainfont{Times New Roman}

% Centering Chapter
\usepackage{sectsty}   
\chapterfont{\centering}

\pagestyle{plain}
\setlength{\parindent}{0.8in}

\usepackage[top=1.5in,bottom=1in,left=1.5in,right=1in]{geometry}



% Theorem style-------------------------------------------------------------------
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{de}[thm]{Definition}
\renewcommand{\proof}{\textbf{Proof.}}

\numberwithin{equation}{chapter} \DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Ima}{Im}

\usepackage{float}
%\usepackage[skip=2pt,font=footnotesize]{caption}

% Graph-------------------------------------------------------------------
\usepackage{graphics,graphicx}
\usepackage{calc}
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
\tikzstyle{vertex}=[circle, draw, inner sep=2pt, minimum size=4pt]
\newcommand{\vertex}{\node[vertex]}
\newcounter{Angle}

% Line space-------------------------------------------------------------------
\renewcommand{\baselinestretch}{1.5}

\newcommand*{\QEDA}{\hfill\ensuremath{\large{\lozenge}}}
\newcommand*{\QEDAa}{\hfill\ensuremath{\square}}

\renewcommand{\bibname}{\centerline{\Large REFERENCES}}

%Centering table of contents and list of table
\usepackage{tocloft}
\renewcommand{\contentsname}{\hfill\bfseries\Large TABLE OF CONTENTS \hfill}   
\renewcommand{\cftaftertoctitle}{\hfill}

\renewcommand{\listtablename}{\hfill\bfseries\Large LIST OF TABLES} 
\renewcommand{\cftafterlottitle}{\hfill}

\renewcommand{\listfigurename}{\hfill\bfseries\Large LIST OF FIGURES} 
\renewcommand{\cftafterloftitle}{\hfill}







\begin{document}
\thispagestyle{empty}

\begin{figure}[h!]
\vskip1in
\begin{center}
\includegraphics[width = 8 cm]{CityU_logo.png}
\end{center}
\end{figure}

\begin{center}
\LARGE{\textbf{Reinforcement Learning Application Based on Unity 3D Engine(Car Racing Game by ML-Agent)}}
\end{center}

\vskip1.5cm

\begin{center}
\textbf{BY}
\end{center}

\vskip1.6cm

\begin{center}
\large{\textbf{Team UFO}}
\end{center}

% \vskip0.6cm


\vskip3.5cm
 

% \vskip2cm
\begin{center}
\textbf{City University Of Hong Kong }
\end{center}


\newpage
\pagenumbering{roman}
\begin{table}[h]
	\begin{tabular}{ll}
		Project Title								   & Reinforcement Learning Application Based on Unity 3D Engine	  \\
										 					&(Car Racing Game by ML-Agent) \\
										 					
		By							   					  & LIU BONAN \\
															& LIU YUZHAN \\
															& PU ZIYI \\
                                                            & TIAN ZONGHANG\\
                                                            & XIE DONG \\
	
	
		Academic Years							  & 2021 \\
\\
	\end{tabular}
\end{table}

\begin{center}
  \large{\textbf{ABSTRACT}}\\
\end{center}
\addcontentsline{toc}{chapter}{ABSTRACT}
The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. In our Deep Reinforcement learning project, we features a obstacle avoiding AI car made with Unity ML Agents. We construct two types obstacle: one is simple static obstacle, and another is random generated ball dropped from the sky. The Agent controls acceleration and steering (both continuous). Observations include the normalized local position, velocity, angular velocity, steering angle, torque and the dot product of the Agent's forward direction and the direction to the next waypoint. 


\noindent {\textbf{Keywords:}} : Deep Reinforcement Learning , ML-Agents, Unity 


\newpage

% \pagenumbering{roman}
%\addcontentsline{toc}{chapter}{LIST OF TABLES}
\tableofcontents
%


\newpage

\pagenumbering{arabic}
\chapter{Introduction and Motivation}
\setlength{\parindent}{0pt}
Deep Reinforcement learning is a commonly used algorism in many area, such as the famous Alpha-Go, real-time navigation, and auto driving. With the high curiosity in the development of auto driving, our group choose to build a project about obstacle avoiding AI car. The object car model we trained not only can run within the vaild route randomly constructed, but also have ability to avoid two types obstacles: one is simple static obstacle, and another is random generated ball dropped from the sky. In order to have a better expressiveness, we use Unity engine to build a 3D model. 


\newpage
\chapter{Methods and Materials}

\section{Deep Reinforcement Learning }\label{Sec2.1}

Deep Reinforcement Learning is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space.Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (eg. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. \\
Various techniques exist to train policies to solve tasks with deep reinforcement learning algorithms, each having their own benefits. At the highest level, there is a distinction between model-based and model-free reinforcement learning, which refers to whether the algorithm attempts to learn a forward model of the environment dynamics.





\section{Unity Machine Learning Agents Toolkit (ML-Agents)}\label{Sec2.2}
The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. The author provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity’s rich environments and then made accessible to the wider research and game developer communities.


\section{Unity 3D engine }\label{Sec2.3}
Unity is a cross-platform game engine developed by Unity Technologies, first announced and released in June 2005 at Apple Inc.'s Worldwide Developers Conference as a Mac OS X-exclusive game engine. As of 2018, the engine had been extended to support more than 25 platforms. The engine can be used to create three-dimensional, two-dimensional, virtual reality, and augmented reality games, as well as simulations and other experiences. The engine has been adopted by industries outside video gaming, such as film, automotive, architecture, engineering and construction. Unity gives users the ability to create games and experiences in both 2D and 3D, and the engine offers a primary scripting API in C$\sharp$, for both the Unity editor in the form of plugins, and games themselves, as well as drag and drop functionality.


\section{Optimization Method }\label{Sec2.4}
\subsection{Proximal Policy Optimization}\label{Sec2.4.1}
Proximal Policy Optimization (PPO) is an on-policy algorithm which can be used for environments with either discrete or continuous action spaces. There are two primary variants of PPO: PPO-Penalty and PPO-Clip.Here, we’ll focus only on PPO-Clip.\\
\textbf{PPO-Clip} doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\\

\textbf{Key Equations} \\
PPO-clip updates policies via
\begin{align*}
    \theta_{k+1}=arg\max_\theta{\ \mathbf{E}_{s,a\sim\pi_{\theta_k}}}\left[L\left(s,a,\theta_k,\theta\right)\right],
\end{align*}
typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here $L$ is given by
\begin{align*}
    L\left(s,a,\theta_k,\theta\right)=min \Big(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s|a),g(\epsilon,A^{\pi_{\theta_k}}(s|a)) \Big) ,
\end{align*}
where 
\begin{align*}
    g(\epsilon,A)=\left\{
    \begin{aligned}
        (1+\epsilon)A,\; A\geq 0  \\
        (1-\epsilon)A,\; A\leq 0   
    \end{aligned}
\right.
\end{align*}
\textbf{Advantage is positive: }Suppose the advantage for that state-action pair is positive, in which case its contribution to the objective reduces to
\begin{align*}
    L\left(s,a,\theta_k,\theta\right)=min\Big(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s|a),(1+\epsilon)\Big)A^{\pi_{\theta_k}}(s|a) ,
\end{align*}
\textbf{Advantage is negative:} Suppose the advantage for that state-action pair is negative, in which case its contribution to the objective reduces to
\begin{align*}
    L\left(s,a,\theta_k,\theta\right)=min \Big(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s|a),(1-\epsilon)\Big)A^{\pi_{\theta_k}}(s|a) ,
\end{align*}

PPO trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima.

\subsection{Soft Actor-Critic Optimization}\label{Sec2.4.2}
Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in an off-policy way, forming a bridge between stochastic policy optimization and DDPG-style approaches. A central feature of SAC is entropy regularization. The policy is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy. This has a close connection to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum.\\
\textbf{Key Equations}\\
Let $x$ be a random variable with probability mass or density function $P$. The entropy $H$ of $x$ is computed from its distribution $P$ according to
\begin{align*}
    H(P)=\mathbf{E}_{x\sim P}[-logP(x)].
\end{align*}
In entropy-regularized reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes the RL problem to:
\begin{align*}
    \pi^{\ast}=arg \max_{\pi}\mathbf{E}_{\tau \sim \pi}\left[\sum^{\infty}_{t=0} \gamma^t \Big (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t))  \Big) \right]
\end{align*}
where $\alpha > 0$ is the trade-off coefficient.We can now define the slightly-different value functions in this setting. $ V^{\pi} $ is changed to include the entropy bonuses from every timestep:
\begin{align*}
     V^{\pi}(s)= \mathbf{E}_{\tau \sim \pi}\left[\sum^{\infty}_{t=0} \gamma^t \Big (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t))  \Big)|s_0=s \right]
\end{align*}
$Q^{\pi}$ is changed to include the entropy bonuses from every timestep except the first:
\begin{align*}
     Q^{\pi}(s,a)= \mathbf{E}_{\tau \sim \pi}\left[\sum^{\infty}_{t=0} \gamma^t \Big (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t))  \Big)|s_0=s,a_0=a \right]
\end{align*}
With these definitions, $V^{\pi}$ and $Q^{\pi}$ are connected by:
\begin{align*}
     V^{\pi}(s)= \mathbf{E}_{\tau \sim \pi}[Q^{\pi}(s,a)]+\alpha H(\pi(\cdot|s_t))
\end{align*}
and the Bellman equation for $Q^{\pi}$ is
\begin{align*}
      Q^{\pi}(s,a)=\mathbf{E}_{s' \sim P}[R(s,a,s')+\gamma V^{\pi}(s')]
\end{align*}
SAC trains a stochastic policy with entropy regularization, and explores in an on-policy way. The entropy regularization coefficient $\alpha$ explicitly controls the explore-exploit tradeoff, with higher $\alpha$ corresponding to more exploration, and lower $\alpha$ corresponding to more exploitation. The right coefficient (the one which leads to the stablest / highest-reward learning) may vary from environment to environment, and could require careful tuning.


\chapter{Result and Discussion}


\chapter{Further Work}

\end{document}
